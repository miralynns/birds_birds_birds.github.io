---
title: "Transferring Data to HPC Cluster"
output:
#  bookdown::pdf_document2:
#    toc: TRUE
  bookdown::html_document2: 
    toc: TRUE
    toc_float: TRUE
    theme: cosmo
link-citations: yes
fontsize: 12pt
editor_options: 
  markdown: 
    wrap: sentence
---

Now that we have organized directories on a cluster, we will move the required files for mining the genome to the cluster.

**Required Files**:

-   Genome of non-model species

-   FASTA file of functional genes of interest (created using the KEGG database)

-   RNA sequence data

# Genome of Non-model species

1.  Identify the non-model species genome using the [NCBI Database](https://www.ncbi.nlm.nih.gov/datasets/taxonomy/8954/)

    -   For this example, the [*Falco peregrinus* reference genome](https://www.ncbi.nlm.nih.gov/datasets/taxonomy/8954/) was used.

2.  Once on the reference genome page, click the 'DOWNLOAD' option (Figure 1.A).
    On the resulting window, select three download options (Figure 1.B):

    1.  Genome Sequences (FASTA)

    2.  Annotation features (GTF)

    3.  Annotation features (GFF)

        ![Figure 1. Genome file downloading options for the Falco peregrinus reference genome as of August 2023. Image A highlights where to click to download. Image B highlights the required files to download.](images/Slide1%203.jpg)

3.  Move all the downloaded genome files into a project folder on the home computer.

    1.  These three files will be needed for the manual annotation process in Geneious after the genome has been mined, but for now, the `.fna.gz` file will need to be transferred onto the computing cluster. It is recommended to use `scp` for this step.

4.  Open a new terminal shell and input the scp command.
    A password will be required to access the files through the HPC before the transfer can begin.

```         
scp <user:/current/path/to/file> <user:/location/where/you/would/like/to/move/file/to>
```

In the example below, all downloaded genome files were transferred to the genome directory using the wildcard `*` option.
This option gathers all files that start with that prefix:

```         
scp /Users/username/Downloads/GCF_023634155.1_bFalPer1.pri_genomic.* HPC-criteria:/scratch/username/tlr_mining/p_falcon/genome
```

# Genes of Interest

1.  Transfer the fasta file created from the KEGG database into the blast directory using `scp`:

```         
scp /Users/username/Downloads/Zebra_Finch_TLRs.fasta HPC-criteria:/scratch/username/tlr_mining/p_falcon/blast
```

2.  Login to the HPC and confirm files have been transferred to the appropriate directories.

# RNA sequence data

## Search the SRA database for RNA-seq data

We will use the [SRA database](https://www.ncbi.nlm.nih.gov/sra) to find RNA sequencing data.
These files will provide protein-level data we can use to mine for the functional genes of interest.
Since are interested in the TLRs, our preferred sample will be blood.

1.  Search the scientific name of the non-model organism (*Falco peregrinus*) of interest in the search bar.
2.  On the resulting page a list of submissions will appear, but we want to focus on RNA source samples. On the left side of the screen select the RNA filter option under 'Source' (Figure 1).
3.  SRA submissions are not consistently labeled. Some might detail the sample type, but that is not always the case. You will want to click on different submissions until you find one from the preferred sample source.
    -   If a sample source is not detailed in the Accession information area, you might need to deep dive into the literature to find the research paper for this sample and review the methods section for sample collection.
    -   In this example, Accession: SRX9708906 (Kola7) was a blood sample and is usable for our workflow.

![Figure 1. SRA database search of RNA source samples for *F. peregrinus*. Top five results displayed from August 2023.](images/Figure_%20SRA_search.jpg)

4.  View the Kola7 SRA information page, we will want to note the run identification so that we can use SRA Toolkit to download the RNA-seq data onto the cluster (Figure 2).

    -   For the Kola7 SRA submission the run identification is SRR13279135
        -   Click on the run number to learn more about the submission and data.

            ![Figure 2. SRA database details for Kola7.](images/Figure_SRA_details.jpg)

Now that we know the Run ID from the SRA database, we will submit a job using SRA Toolkit to download the RNA-seq files to the cluster. Next we will need to download the relevant SRA files.

## Use SRA Toolkit to download RNA-seq data to the cluster

[SRA Toolk](https://github.com/ncbi/sra-tools) is a module created to download SRA data.
Most data is too large to download directly from the SRA repository, so this toolkit is usually a necessary step.

1.  Create a job file in the terminal using the `nano` command

    `nano sra_pfalcon`

2.  Adjust the slurm script example below and copy it into the nano screen.

3.  Run the slurm script, using the `sbatch` command:

    `sbatch sra_pfalcon`

4.  Once the job is finished running, confirm there are two files in the sra directory.

    -   For this example, the resulting files should be SRR13279135_1.fastq and SRR13279135_2.fastq.

### SRA toolkit Slurm script example:

```         
#!/bin/bash
#SBATCH -J sra_pfalcon
#SBATCH -o log.o%j 
#SBATCH -p bsudfq
#SBATCH -N 1
#SBATCH -n 14
#SBATCH -t 14-00:00:00
#SBATCH --mail-type=END
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=emailaddress

#########################
# Load Modules:
module load bioinformatics/2023-05-04
module load sratoolkit/2.10.9

# Sort Path:
datapath=/bsuscratch/username/tlr_mining/p_falcon/sra
## Sort Path section is required if you are creating a jobfile to submit a job from a different directory than the directory where the required files for the job are located.

# SRA toolkit:
fastq-dump -I --split-files SRR13279135
```

**Note:** Be mindful of module versions available on your cluster.
If you receive an Error Code 3, then the SRA file is rather large you will need to use `prefetch` and `fasterq-dump`.
See the example below.

```         
prefetch SRR21639343 --max-size 96000000000
fasterq-dump SRR21639343 --split-files
```

#### Dependencies Explanation:

```         
- max-size: prefetch has a 20 GB maximum, so the limit will need to be manually increased for accession requests larger than 20 GB
- split-flies: sra files for the gfalcon were too big so I had to use prefetch. This required some configuration and then the sbatch file.
```
